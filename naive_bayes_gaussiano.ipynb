{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Gaussiano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) es un dataset con imágenes de prendas de vestir (artículos de la compañía Zalando). Son 70000 imágenes en baja resolución (28x28 pixeles), en escala de grises, agrupadas en 10 clases. Se utiliza en la comunidad de Machine Learning para comparación de algoritmos, de la misma forma que el dataset MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml \n",
    "X, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta práctica es utilizar la técnica de Naive Bayes gaussiano como clasificador de imágenes, siguiendo la notebook para obtener las respuestas del cuestionario en Campus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "                '0':'T-shirt/top',\n",
    "                '1':'Trouser',\n",
    "                '2':'Pullover',\n",
    "                '3':'Dress',\n",
    "                '4':'Coat',\n",
    "                '5':'Sandal',\n",
    "                '6':'Shirt',\n",
    "                '7':'Sneaker',\n",
    "                '8':'Bag',\n",
    "                '9':'Ankle boot'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribución de clases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿El dataset está balanceado? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Redimensionar los datos de entrada con la dimensión de las imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graficar las primeras 150 imágenes como una grilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graficar las primeras 100 imágenes de cada clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split train-validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dividir los datos en train, validation y test, en un ratio 5:1:1, usando **random_state=42**. De forma estratificada por clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequear que quedaron balanceados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAIN')\n",
    "print(pd.Series([label_dict[label] for label in y_train]).value_counts(normalize=True))\n",
    "\n",
    "print('VALIDATION')\n",
    "print(pd.Series([label_dict[label] for label in y_val]).value_counts(normalize=True))\n",
    "\n",
    "print('TEST')\n",
    "print(pd.Series([label_dict[label] for label in y_test]).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Gaussiano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis pixel por pixel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede modelar cada pixel como una variable aleatoria continua, cuyo dominio está entre 0 y 255. En ese caso, cada observación tendrá un conjunto de features: \n",
    "\n",
    "$X = [ X_{(0,0)},X_{(0,1)},...,X_{(0,27)},X_{(1,0)},...,X_{(27,27)} ]$\n",
    "\n",
    "donde $X_{(i,j)}$ es la variable aleatoria del valor de gris del pixel de coordenadas i,j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *¿Cuántas variables aleatorias se samplean para generar una imagen?* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teorema de Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar el Teorema de Bayes es necesario conocer el **likelihood** de cada $X$ dada la clase $y$, es decir $P(X|Y)$\n",
    "\n",
    "Por ejemplo, si se representa una imagen negra como un vector $v=[0,...,0]$ de 784 ceros. Entonces $P(X=v|Y=1)$ es el likelihood de que una imagen de clase 'Trouser' sea una imagen negra. \n",
    "\n",
    "Por otro lado, la probabilidad a posteriori $P(Y=1|X=v)$ es la probabilidad de que una imagen negra pertenezca a la clase 'Trouser'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se puede estimar $P(X|Y)$ para cada clase y para cada posible combinación de píxeles? Si se asume que hay independencia entre las variables aleatorias, se puede calcular el likelihood de $X$ como el producto de los likelihoods $X_{i,j}$ individuales: \n",
    "\n",
    "$P(X|Y) = P(X_{(0,0)}|Y) P(X_{(0,1)}|Y) ... P(X_{(27,27)}|Y) \\rightarrow$   *Naive Bayes*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribución gaussiana**\n",
    "\n",
    "Como se vio en la práctica anterior, el clasificador de Naive Bayes resulta útil para el caso de datos discretos, como el caso de la moneda o el clasificador de artículos. Para utilizarlo con datos continuos habría que hacer una binarización de los datos y eso usualmente aumenta mucho la cantidad de parámetros. \n",
    "\n",
    "Otra opción es asumir que los valores tienen una distribución gaussiana, en cuyo caso se separan los datos en las $K$ clases que se quieren diferenciar y se obtienen las medias y varianzas de cada clase. \n",
    "\n",
    "Si se tiene una observación $v$ que se quiere clasificar, se procede a calcular con cual de las $k$ distribuciones es más probable que la observación haya sido generada.\n",
    "\n",
    "$p(x=v|Y_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}e^{-\\frac{(v-\\mu_k)²}{2\\sigma_k^2}} \\rightarrow $ *Distribución gaussiana*\n",
    "\n",
    "Dado que cada pixel es una variable aleatoria continua, se puede modelar como una distribución normal. \n",
    "\n",
    "- ¿Cuál es la media y el desvío estándar del pixel (10,10) en la clase 'Trouser' estimados a partir del set de train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graficar la distribución normal del pixel (10,10) de la clase 'Trouser' y de la clase 'Pullover'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *¿Cuál es la media y el desvío estándar del pixel (14,14) en la clase 'Bag' estimados a partir del set de train?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Armar una matriz con la media y el desvío para cada pixel y para cada clase. Tendrá dimensión (2x784x10)\n",
    "- Escribir una función para el plot de una gaussiana\n",
    "- Graficar las distribuciones del pixel (10,10) para las 10 clases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Priori, likelihood y posteriori**\n",
    "- *¿Cuál es el likelihood de observar en las imágenes de 'Trouser' un valor de gris 145 en el pixel (10,10)? Estimado con el set de train*\n",
    "- Calcule las probabilidades a priori de las 10 clases\n",
    "- *Con los dos resultados anteriores, calcule la probabilidad a posteriori no normalizada de que un valor de 145 observado en el pixel (10,10) pertenezca a una imagen de Trouser*\n",
    "- *Según el criterio de maximum likelihood, ¿qué clase es más probable que haya generado una imagen con un valor de gris 145 en el pixel (10,10)?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complejidad del modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- *¿Cuántos parámetros tendrá el modelo de Naive Bayes Gaussiano dado que se asumió independencia entre los píxeles?*\n",
    "- *Si no se asume independencia, es decir que las covarianzas entre variables no se desprecian: ¿cuántos parámetros tendría el modelo?* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Gaussiano como clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ajustar un modelo de NB gaussiano a los datos de train. Probar ajustar el hiperparámetro de **smoothing** con los valores indicados. Considerando el accuracy promedio entre clases, ¿cuál es el mejor accuracy que obtuvo en validación? ¿Con qué valor de smoothing lo obtuvo? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_smoothings = [1,1e-1,1e-3,1e-5,1e-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clasificar los datos de test con el modelo seleccionado. Reportar accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlación entre píxeles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, asumir independencia entre variables significa asumir que un pixel no tiene relación con su pixel vecino. Por supuesto esto es un supuesto erróneo en el caso de imágenes, donde los patrones visuales se forman gracias a la relación entre los píxeles. \n",
    "\n",
    "- ¿Cómo se correlaciona el valor de un pixel con sus vecinos? Calcule la matriz de autocorrelación de cada pixel con su entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agrupar píxeles como hiperparámetro**\n",
    "\n",
    "Considerando ahora que cada vecindad de 4 píxeles pertenece a una misma distribución. Es decir, agrupando 4 píxeles y representando cada grupo con una única media y un único desvío estándar. \n",
    "\n",
    "- Si ajustásemos un modelo de NB gaussiano con esta nueva representación de variables. ¿Cuántos parámetros tendría el modelo? \n",
    "\n",
    "<img src=\"groupGaussians.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ajustar un modelo Naive Bayes Gaussiano agrupando los píxeles de a cuatro. ¿Qué accuracy obtiene en validación con un var_smoothing de 0.1? \n",
    "\n",
    "Una opción es implementar sin utilizar el paquete sklearn. Para esto, se debe calcular las medias y los desvíos de las distribuciones gaussianas. Luego, preparar una función que calcule el likelihood de una clase para una imagen como el producto del likelihood del valor de gris de cada pixel de la imagen, valuando la distribución gaussiana que corresponda. Finalmente, se selecciona por maximum likelihood la clase más probable. Aplicar la función a las imágenes de validación y calcular el accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Otra opción es acomodar el dataset para poder usar el método de sklearn. Separamos la imagen en tantas matrices como grupos de pixeles se tengan. Cada nueva matriz tendrá una muestra de cada gaussiana. \n",
    "\n",
    "<img src=\"divideImage.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupos de 2x2 pixeles\n",
      "Cada imagen se convirtió en 4 matrices\n",
      "Tenemos 50000 observaciones, con 4 matrices de 14x14 en cada observacion\n"
     ]
    }
   ],
   "source": [
    "pixels_per_group = 4\n",
    "X_train = X_train.reshape(-1,28,28)\n",
    "\n",
    "X_train_full = ...\n",
    "print(f'Cada imagen se convirtió en {len(X_train_full)} matrices' )\n",
    "print(f'Tenemos {X_train_full[0].shape[0]} observaciones, con {len(X_train_full)} matrices de {X_train_full[0].shape[1]}x{X_train_full[0].shape[2]} en cada observacion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos cada matriz en una observación distinta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 200000 observaciones de dimensión 14x14\n"
     ]
    }
   ],
   "source": [
    "X_train_full = ...\n",
    "print(f'Tenemos {X_train_full.shape[0]} observaciones de dimensión {X_train_full.shape[1]}x{X_train_full.shape[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora repetimos cada pixel cuatro veces, para tener matrices de 28x28 nuevamente\n",
    "<img src=\"imageTo4Images.png\" width=\"480\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 784)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full = X_train_full.reshape(y_train_full.shape[0],-1)\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=0.1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian = GaussianNB(var_smoothing=0.1)\n",
    "gaussian.fit(X_train_full,y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Búsqueda de hiperparámetros**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Realizar una búsqueda de hiperparámetros tipo grilla utilizando los siguientes valores de hiperparámetros:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_smoothings = [1,1e-1,1e-3,1e-5,1e-7]\n",
    "pixels_per_group = [1,4,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota: Los ajustes con pixels_per_group=1 corresponden al ejercicio anterior. El pixels_per_group=14 se refiere a un rectángulo de 7x2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calcular cuántos parámetros tiene el modelo para cada valor de pixels_per_group\n",
    "- ¿Qué combinación de hiperparámetros obtuvo mejor accuracy en validación? \n",
    "- Clasificar los datos de test con el modelo seleccionado y reportar accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
